\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{titlesec}
\usepackage{array}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{listings}
\usepackage{xcolor}

\geometry{
    a4paper,
    left=1in,
    right=1in,
    top=1in,
    bottom=1in
}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Bank Customer Segmentation},
    pdfauthor={Data Scientist},
    pdfsubject={Bank Customer Segmentation},
    pdfkeywords={Machine Learning, Bank Data, Clustering, Unsupervised Learning}
}

\titlelabel{\thetitle.\quad}
\titleformat{\section}{\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{\thesubsection}{1em}{}
\titleformat{\subsubsection}{\normalfont\normalsize\bfseries}{\thesubsubsection}{1em}{}

\begin{document}

\begin{titlepage}
    \centering
    \vspace*{2cm}
    {\LARGE\bfseries Bank Data Clustering and Customer Segmentation\par}
    \vspace{2cm}
    {\Large Predicting customer category according to their info.\par}
    \vspace{3cm}
    {\Large Shoyeb Ansari\par}
    \vspace{1cm}
    {\large\today\par}
    \vfill
    {\large\textbf{GitHub Repository:} \url{https://github.com/Shoyeb45/CustomerSegmentation}\par}
\end{titlepage}

\thispagestyle{empty}
\tableofcontents
\pagebreak

\listoffigures
\pagebreak

\listoftables
\pagebreak


\section{Introduction}
This report presents a comprehensive analysis of customer Segmentation of bank data. The main objective was to build machine learning models to categorise the customer in different clusters. 

\subsection{Project Scope and Objectives}
The primary goals of this project were:
\begin{itemize}
    \item To perform thorough exploratory data analysis on the bank dataset
    \item To build and compare different Clustering models for categorising user behavior
    \item To select the optimal model for customer Segmentation
\end{itemize}


\section{Data Ingestion}

The data is imported using pandas. Using $\text{read\_csv()}$ function.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{reading_data.png}
    \caption{Reading data}
    \label{fig:example}
\end{figure}

\subsection{Dataset Overview}
The dataset consists of 1525 voter records with 9 variables. These variables include various demographic and behavioral attributes that could potentially influence voting decisions.

\subsection{Null Value Analysis and Unnecessary Columns}
A thorough check for missing values was conducted to ensure data quality and completeness before proceeding with the analysis.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{data_info.png}
    \caption{Data information}
    \label{fig:example}
\end{figure}

\section{Exploratory Data Analysis}

\subsection{Descriptive Statistics}
The initial inspection of the dataset revealed the following characteristics:

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{data_stats.png}
    \caption{Data Statistics}
    \label{fig:example}
\end{figure}


\subsection{Univariate Analysis}
Univariate analysis was performed to understand the distribution of individual variables and identify potential anomalies.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{univariate_analysis.png}
    \caption{Distribution of all features}
    \label{fig:example}
\end{figure}

As here we can see that there are different type of ranges of the values of different features. And each of the features have different scales. In order to do better clustering, we need to have a fair distance metric, so we need to normalise the features here. 


We can use either of these technique:
\begin{itemize}
    \item \textbf{Standardization (Z-score scaling)}: Centers the data around 0 with a standard deviation of 1.
    \item \textbf{Normalization (Min-Max scaling)}: Scales values between 0 and 1.
\end{itemize}


\subsection{Bivariate Analysis}

Bivariate analysis was conducted to examine relationships between variables and particularly to understand how different factors correlate with the target variable .


\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{bivariate_analysis.png}
    \caption{Correlation coefficient heat map}
    \label{fig:example}
\end{figure}

We can observe following observation :

\begin{enumerate}
    \item 1. Highly Correlated Features (Multicollinearity)

    \begin{itemize}
        \item spending \& advance\_payments (0.99)
        \item spending \& current\_balance (0.95)
        \item spending \& credit\_limit (0.97)
        \item advance\_payments \& current\_balance (0.97)
        \item advance\_payments \& credit\_limit (0.94)
        \item current\_balance \& credit\_limit (0.86)
        \item current\_balance \& max\_spent\_in\_single\_shopping (0.93)
    \end{itemize}
    

    \item Moderate Correlation: 
    
    \begin{itemize}
        \item probability\_of\_full\_payment \& credit\_limit (0.76)
        
        \item probability\_of\_full\_payment \& spending (0.60)
        
    \end{itemize}

    \item Weak or No Correlation:

    \begin{itemize}
        \item min\_payment\_amt has weak or negative correlations with other features.
    \end{itemize}
\end{enumerate}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{bi_scatter.png}
    \caption{Scatter plot between all features}
    \label{fig:example}
\end{figure}

Impact on Clustering

\begin{enumerate}
    \item \textbf{Redundant Features}: Since spending, advance\_payments, current\_balance, and credit\_limit are highly correlated, they may introduce bias and cause clusters to be formed based on redundant information.
    
    \item \textbf{Over-Influence of Strongly Correlated Features}: K-Means and other distance-based clustering methods give more weight to features with larger variance. Highly correlated features can dominate the clustering process, leading to misleading groupings.


    \item \textbf{Multicollinearity Issues}: Clustering algorithms assume independent features. Highly correlated features can create misleading distance calculations.
\end{enumerate}


How to solve?

- We'll use Feature Selection to solve this problem of highly multicollinearity. We'll see that in PreProcessing step


\subsection{Outlier Detection}

The dataset was examined for potential outliers that could affect model performance by visualizing box-plot of features.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{box_plot.png}
    \caption{Box plot of all features.}
    \label{fig:example}
\end{figure}


\section{Data Pre Processing}

\subsection{Standardization}

As we seen that the ranges and scales are not proper, so we have to scale the data to do justice to distance metrices.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{code_Scaling.png}
    \caption{Code to perform the Standardization}
    \label{fig:example}
\end{figure}


\subsection{Outlier Treatmant}

According to the boxplot, we had only two columns for outliers. So here's the function to treat the outliers:


\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{outlier_treatmant.png}
    \caption{Outlier treatmant code}
    \label{fig:example}
\end{figure}

Columns which conatins outliers :

\begin{itemize}
    \item probability\_of\_full\_payment
    \item min\_payment\_amt
\end{itemize}

\end{document}
</antArtifact